Algorithm Structure (further analysis in Write Up.docx)

- For our pipeline, we use the Q1500US bundle, a price floor, and a liquidity floor to create our base universe. 
- This base universe may not be optimal because it only contains up to 30% of a given sector, but we didn’t see much of a difference from our original filter.
- Our pipeline generates stock rankings based off of the Starr Ratio (excess returns divided by expected shortfall) over a 252-day trailing period.
- We tried anywhere from 89 days to 504 days, but we only really saw favorable returns around the one-year range.
- We then go long on the top 2.5% of the stocks and short the bottom 2.5% of the stocks.
- We fooled around with these percentages a bit but selecting too few often skewed our betas and selecting too many reduced our alphas.
- We allocate the same percentage of our portfolio to each stock.  
- This could have been why our original Sharpe Ratio idea failed. In theory, we could have done a Markowitz standard mean variance portfolio optimization to fix it, but we thought that using Starr Ratio was simpler.
- Our pipeline also calculates the market correlation of each stock with SPY over the last 100 days using a 30-day return window.
- To ensure a low market correlation, we calculate the net beta of our portfolio after we select our stock positions and then buy or sell SPY to offset our exposure.
- Because we both longed and shorted stocks for our strategy, we targeted 1.00 and -1.00 as portfolio percentages for our long and short positions, respectively.
- We needed to do this to increase our time weighted cash position. We also could have increased our long positions and reduced our short positions, but this consistently yielded poor outcomes. This resulted in our leverage hovering around ~2 most of the time. 

Progression of Algorithm

- I have included the main research environment file that we used for creating our algorithm. Because our algorithm focused almost completely on stock selection, you can see much of the work that we did from this environment. Originally, we used the AvgRet custom factors to create ranks for our momentum strategy based on average daily returns. This failed, so we used the AvgRet and StdDev custom factors to create rankings for a Sharpe Ratio momentum strategy, but that also failed. We also had a custom factor to manually calculate market correlation for each stock, but it took too long to compute anything with that so we settled for the built in Quantopian factor. - When we switched to using Starr Ratio (excess returns divided by expected shortfall), things became a bit trickier. Although we were able to create a custom class to calculate CVaR for a given cutoff, this calculation ran extremely slowly on Quantopian. Because of this, we had to significantly shrink our stock universe so that we could complete backtests, but even then backtests longer than three months would often give timeout errors. By doing this, we also ran the risk of forgoing potential returns and holding a less diverse portfolio. 
- For this reason, we decided to create a custom factor to approximate CVaR. In order to do this, we took a simple nanpercentile calculation instead of averaging all of a stock’s returns below a certain percentile. This made our calculations must faster because we simply had to perform one calculation across an axis instead of iterating through the columns of a data frame and taking the conditional means along those columns. This allowed us to increase our stock universe and complete backtests in a much smaller amount of time.
- However, it must be noted that by making this change, we probably sacrificed some degree of precision of our CVaR in order to gain computation speed. This is because instead of calculating the individual CVaR for each stock, we calculated a fixed percentile of each stock’s returns that would closely imitate the stock’s expected shortfall. However, we attempted to fix this problem by comparing the CVaR and the approximation (named AVaR in our research environment). In order to do this, we calculated each one for recent data in the pipeline and chose a fixed percentile for AVaR that would minimize the average difference between AVaR and CVaR when we ran the pipeline. For example, a 5% CVaR corresponded to an AVaR of 1.5% with an average difference of 0.11%, and a 10% CVaR corresponded to an AVaR of 3% with an average difference of 0.17%. Thus, this simplification probably cost us some degree of precision, but we would not be able to explore much of our algorithm’s performance without this sacrifice. 
- For the final version of our algorithm, we chose to optimize our CVaR approximation. We ended up using a AVaR percentile of 1, which would have roughly corresponded to a CVaR of around 3%. In the future, it would be nice if we had a platform powerful enough both to calculate CVaR and backtest a larger amount of stocks. Because our strategy focuses on stock selection, this made optimization in research infeasible because we would have to severely limit our stock universe so that we could load all of the appropriate data into research. Furthermore, we believe that approximating CVaR may have inconsistency for our algorithm’s performance, but as of now we have no way to test this. Because of this and the possible regime shift which may have affected our algorithm, we tried to optimize both with regards to the past couple months of data and some historical data for December. We hope that this will help to counter any potential seasonality that may be affecting our strategy.
- Overall, we’re pleased with the outcome of our project. Although consistent strategy performance proved to be illusive, we successfully implemented all of the ideas and improvements that we had in one way or another. 



